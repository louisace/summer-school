{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;&ensp;逻辑回归是一个用于分类的线性模型，该模型的输出变量始终位于0-1之间。模型假设为：$h_{\\theta}(x)=g(\\theta^{T}X)$。其中，X代表特征向量，g代表逻辑函数，公式为：$g(z)=\\frac{1}{1+e^{-z}} $。$ h_{\\theta}(x) $可以理解为：对与给定的输入变量 $ x_{i} $，计算得到的输出为1的概率，即 $ h_{\\theta}{x}=P(y=1|x;\\theta) $\n",
    "<br />\n",
    "&ensp;&ensp;&ensp;&ensp;逻辑回归的损失函数为：\n",
    "<center>$ J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}Cost(h_{\\theta}(x_{(i)}, y_{(i)}) $</center>\n",
    "其中：\n",
    "<center>$ Cost(h_{\\theta}(x_{(i)}, y_{(i)}) = \\left\\{\\begin{matrix} &-log(h_{\\theta}(x)) & if & y=1 \\\\ &-log(1-h_{\\theta}(x))  & if  & y=0  \\end{matrix}\\right.$</center>\n",
    "&ensp;&ensp;&ensp;&ensp;此时损失函数的特点是，当实际分类为y=1且$h_{\\theta}(x)=1$的损失为0，当y=1但$h_{\\theta}(x) \\neq 1$的误差损失会随着$h_{\\theta}(x)$的变小而变大；当实际分类y=0且$h_{\\theta}(x)=0$的损失为0，当y=1但$h_{\\theta}(x) \\neq 0$的误差损失会随着$h_{\\theta}(x)$的变大而变大。\n",
    "&nesp;&nesp;&nesp;&nesp;利用极大似然估计对模型似然估计，此时的损失函数可简化为：\n",
    "<center>$Cost(h_{\\theta}(x_{(i)}, y_{(i)}) = -y \\times -log(h_{\\theta}(x)) + (1-y) \\times -log(1-h_{\\theta}(x))$</center>\n",
    "&ensp;&ensp;&ensp;&ensp;此时的损失函数为：\n",
    "<center>$ J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}[ -y \\times -log(h_{\\theta}(x)) + (1-y) \\times -log(1-h_{\\theta}(x))]$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def cost(theta, x, y):\n",
    "    theta = np.matrix(theta)\n",
    "    x = np.matrix(x)\n",
    "    y = np.matrix(y)\n",
    "    a = np.multiply(-y, np.log(sigmoid(x * theta.T)))\n",
    "    b = np.multiply((1-y), np.log(1-sigmoid(x * theta.T)))\n",
    "    return np.sum(a-b) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;&ensp;&ensp;&ensp;使用梯度下降法对损失函数求解，求得使损失函数最小的参数，求解算法为：\n",
    "<center>$ \\theta_{j} = \\theta_{j} - \\alpha \\frac{1}{m} \\sum(h_{\\theta}(x^{(i)})-y^{(i)})$</center>\n",
    "重复上述操作，使其最后的梯度为0则得到了最优的参数解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient\n",
    "def gradient(theta, x, y):\n",
    "    grad = np.zeros(theta.shape)\n",
    "    for j in range(len(theta.ravel())): #for each parmeter\n",
    "        hx = sigmoid(x, theta.T)\n",
    "        error = hx - y\n",
    "        term = np.multiply(error, X[:,j])\n",
    "        grad[0, j] = np.sum(term) / len(X)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute decent\n",
    "def decent(theta, x, y, lr, thresh):\n",
    "    while True:\n",
    "        grad = np.zeros(theta.shape)\n",
    "        cost = cost(theta, x, y)\n",
    "        grad = gradient(theta, x, y)\n",
    "        theta = theta - lr * grad\n",
    "        if np.linalg.norm(grad) < threshold:\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST-original', data_home='./dataSet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn中LogisticRegression库函数的参数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| parameters | means |\n",
    "| ---- | ---- |\n",
    "| penalty | 'l1','l2','elasticnet' or 'None',默认为l2正则，指定惩罚中的范数 |\n",
    "| dual | 可选项，默认为False。对偶或原始方法。对偶公式只适用于l2罚的线性解算。当n_samples > n_features时，首选dual=False。 |\n",
    "| tol| 可选项，默认为1e-4。迭代终止的阈值 |\n",
    "| C | 可选项，默认为1.0。C为正则化系数$\\lambda$的倒数 |\n",
    "|fit_intercept| 可选项，默认为True。指定是否应该向决策函数添加常量(也称为偏差或截距)。 |\n",
    "| intercept_scaling | 可选项，默认为1。仅在使用求解器“liblinear”且self.fit_intercept设置为True时有用。 |\n",
    "| class_weight | 可选项，默认为None。与{class_label: weight}形式的类关联的权重。如果没有给出，所有的类都应该有权重1 |\n",
    "| random_state | 可选项，默认为None。伪随机数生成器的种子，用于在混洗数据时使用。 如果是int，则random_state是随机数生成器使用的种子; 如果是RandomState实例，则random_state是随机数生成器; 如果为None，则随机数生成器是np.random使用的RandomState实例。在求解器为'sag'或'liblinear'时使用。|\n",
    "|solver| 可选项， {newton-cg',lbfgs','liblinear','sag','saga'}，默认为'liblinear',用于优化问题的算法。对于小规模的数据，'liblinear'是一个更好的选择，'sag'和'saga'对于大规模数据会更好。 |\n",
    "| max_iter | 可选项，默认为100。表示最大迭代次数 |\n",
    "| multi_class | 可选项，参数为：{'ovr','multinomial','auto'},默认为'ovr'.如果选择的选项是“ovr”，那么一个二进制问题适合于每个标签，否则损失最小化就是整个概率分布的多项式损失。对liblinear solver无效。 |\n",
    "| verbose | 可选项，默认为0.对于liblinear和lbfgs求解器，将verbose可以设置为任何正数。 |\n",
    "| warm_start | 可选项，默认为False.当设置为True时，重用前一个调用的解决方案以适合初始化。否则，只擦除前一个解决方案。对liblinear解码器无效。 |\n",
    "| n_jobs | 可选项，默认为None。如果multi_class='ovr' ，则为在类上并行时使用的CPU核数。当将'solver'设置为'liblinear'时，无论是否指定了multi_class，将忽略此参数。如果给定值为-1，则使用所有核。|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
