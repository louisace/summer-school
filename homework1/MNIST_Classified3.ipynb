{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import mnist\n",
    "from torch import nn,optim\n",
    "from torch.autograd import variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x-0.5)/0.5\n",
    "    x = x.reshape((-1, 1))\n",
    "    x = torch.from_numpy(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist.MNIST('./dataSet', train=True, transform=tf, download = True)\n",
    "test_set = mnist.MNIST('./dataSet', train=False, transform=tf, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 400), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(400, 200), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(200, 100), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(100, 10), \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net = net.to(device)\n",
    "net = net().cuda()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, Train Loss:0.012784, Train Acc:0.997518, Eval Loss:0.100218, Eval Acc:0.970629\n",
      "epoch:1, Train Loss:0.012330, Train Acc:0.997601, Eval Loss:0.071630, Eval Acc:0.979035\n",
      "epoch:2, Train Loss:0.011084, Train Acc:0.998118, Eval Loss:0.075868, Eval Acc:0.979529\n",
      "epoch:3, Train Loss:0.010515, Train Acc:0.998268, Eval Loss:0.076647, Eval Acc:0.978244\n",
      "epoch:4, Train Loss:0.009866, Train Acc:0.998368, Eval Loss:0.074194, Eval Acc:0.979134\n",
      "epoch:5, Train Loss:0.008980, Train Acc:0.998867, Eval Loss:0.081542, Eval Acc:0.978244\n",
      "epoch:6, Train Loss:0.008740, Train Acc:0.998651, Eval Loss:0.100614, Eval Acc:0.971717\n",
      "epoch:7, Train Loss:0.008310, Train Acc:0.998834, Eval Loss:0.078363, Eval Acc:0.979134\n",
      "epoch:8, Train Loss:0.007499, Train Acc:0.999134, Eval Loss:0.081921, Eval Acc:0.978046\n",
      "epoch:9, Train Loss:0.006935, Train Acc:0.999150, Eval Loss:0.075185, Eval Acc:0.978145\n",
      "epoch:10, Train Loss:0.006570, Train Acc:0.999434, Eval Loss:0.076062, Eval Acc:0.979134\n",
      "epoch:11, Train Loss:0.006100, Train Acc:0.999400, Eval Loss:0.077436, Eval Acc:0.979035\n",
      "epoch:12, Train Loss:0.005871, Train Acc:0.999467, Eval Loss:0.084813, Eval Acc:0.977453\n",
      "epoch:13, Train Loss:0.005377, Train Acc:0.999567, Eval Loss:0.077923, Eval Acc:0.979826\n",
      "epoch:14, Train Loss:0.005074, Train Acc:0.999634, Eval Loss:0.083507, Eval Acc:0.977057\n",
      "epoch:15, Train Loss:0.004797, Train Acc:0.999567, Eval Loss:0.079211, Eval Acc:0.979331\n",
      "epoch:16, Train Loss:0.004627, Train Acc:0.999667, Eval Loss:0.079022, Eval Acc:0.979134\n",
      "epoch:17, Train Loss:0.004285, Train Acc:0.999650, Eval Loss:0.078325, Eval Acc:0.980123\n",
      "epoch:18, Train Loss:0.004040, Train Acc:0.999700, Eval Loss:0.078398, Eval Acc:0.979430\n",
      "epoch:19, Train Loss:0.003731, Train Acc:0.999850, Eval Loss:0.079989, Eval Acc:0.979035\n",
      "epoch:20, Train Loss:0.003635, Train Acc:0.999750, Eval Loss:0.078744, Eval Acc:0.979925\n",
      "epoch:21, Train Loss:0.003352, Train Acc:0.999850, Eval Loss:0.085943, Eval Acc:0.978540\n",
      "epoch:22, Train Loss:0.003168, Train Acc:0.999883, Eval Loss:0.082474, Eval Acc:0.978145\n",
      "epoch:23, Train Loss:0.003048, Train Acc:0.999817, Eval Loss:0.085027, Eval Acc:0.979035\n",
      "epoch:24, Train Loss:0.002963, Train Acc:0.999900, Eval Loss:0.083050, Eval Acc:0.978738\n",
      "epoch:25, Train Loss:0.002708, Train Acc:0.999917, Eval Loss:0.083593, Eval Acc:0.978540\n",
      "epoch:26, Train Loss:0.002597, Train Acc:0.999900, Eval Loss:0.086106, Eval Acc:0.978343\n",
      "epoch:27, Train Loss:0.002468, Train Acc:0.999950, Eval Loss:0.082447, Eval Acc:0.979233\n",
      "epoch:28, Train Loss:0.002374, Train Acc:0.999933, Eval Loss:0.083908, Eval Acc:0.978936\n",
      "epoch:29, Train Loss:0.002248, Train Acc:0.999950, Eval Loss:0.084174, Eval Acc:0.979233\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "for e in range(30):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net.train()\n",
    "    for im, label in train_data:\n",
    "        im = im.view(im.size(0), -1)\n",
    "        im = im.to(device)\n",
    "        label = label.to(device)\n",
    "        # 前向传播\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 记录误差\n",
    "        train_loss += loss.item()\n",
    "        # 计算分类的准确率\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        train_acc += acc\n",
    " \n",
    "    losses.append(train_loss / len(train_data))\n",
    "    acces.append(train_acc / len(train_data))\n",
    "    # 在测试集上检验效果\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    net.eval()\n",
    "    for im, label in test_data:\n",
    "        im = im.view(im.size(0), -1)\n",
    "        im = im.to(device)\n",
    "        label = label.to(device)\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 记录误差\n",
    "        eval_loss += loss.item()\n",
    "        # 记录准确率\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().item()\n",
    "        acc = num_correct / im.shape[0]\n",
    "        eval_acc += acc\n",
    "    \n",
    "    eval_losses.append(eval_loss / len(test_data))\n",
    "    eval_acces.append(eval_acc / len(test_data))\n",
    "    print('epoch:{}, Train Loss:{:.6f}, Train Acc:{:.6f}, Eval Loss:{:.6f}, Eval Acc:{:.6f}'\n",
    "          .format(e, train_loss/len(train_data), train_acc / len(train_data),\n",
    "                 eval_loss / len(test_data), eval_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
